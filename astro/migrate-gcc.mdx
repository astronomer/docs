---
sidebar_label: "From Google Cloud Composer"
title: "Migrate to Astro from Google Cloud Composer"
id: migrate-gcc
description: Get started on Astro by migrating your Airflow code from Google Cloud Composer (GCC).
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

import Prerequisites from "../docs-partials/prereqs.mdx";
import StarshipUsage from "../docs-partials/starship-usage.mdx";
import Workspaces from "../docs-partials/workspaces.mdx";
import Deployments from "../docs-partials/deployments.mdx";
import DirectorySetup from "../docs-partials/directory-setup.mdx";
import FinalStep from "../docs-partials/final-step.mdx";
import Deploy from "../docs-partials/deploy.mdx";

This is where you'll find instructions for migrating an Airflow environment from [Google Cloud Composer (GCC)](https://cloud.google.com/composer/docs/concepts/overview) to Astro.

To complete the migration process, you will:

- Prepare your source Airflow and set up your Astro Airflow environment.
- Migrate metadata from your source Airflow environment.
- Migrate DAGs and additional Airflow components from your source Airflow environment.
- Complete the cutover process to Astro.

## Prerequisites

<Prerequisites />

You can additionally use the `gcloud` CLI to expedite some steps in this guide.

## Step 1: Install Astronomer Starship

The [Astronomer Starship](https://pypi.org/project/astronomer-starship/) migration utility connects your source Airflow environment to your Astro Deployment and migrates your Airflow connections, Airflow variables, environment variables, and DAGs.

The Starship migration utility works as a [plugin](https://github.com/astronomer/starship/tree/master/astronomer-starship) with a user interface, or as an Airflow operator if you are migrating from a more restricted Airflow environment.

See the following table for information on which versions of Starship are available depending on your source Airflow environment:

| Source Airflow environment     | Starship plugin | Starship operator |
|--------------------------------|-----------------|-------------------|
| Airflow 1.x                    | ❌               | ❌                 |
| Cloud Composer 1 - Airflow 2.x |                 | ✔️️               |
| Cloud Composer 2 - Airflow 2.x | ✔️️             |                   |

To install the Starship plugin on your Cloud Composer 1 or Cloud Composer 2 instance, install the `astronomer-starship` package in your source Airflow environment. See [Install packages from PyPI](https://cloud.google.com/composer/docs/composer-2/install-python-dependencies#install-pypi)

:::tip

You can alternatively complete this installation with the `gcloud` CLI by running the following command:

```sh
gcloud composer environments update [GCC_ENVIRONMENT_NAME] \
    --location [LOCATION] \
    --update-pypi-package=astronomer-starship
```

:::

## Step 2: Create an Astro Workspace (Optional)

<Workspaces />

## Step 3: Create an Astro Deployment (Optional)

<Deployments />

## Step 4: Use Starship to Migrate Airflow Connections and Variables

<StarshipUsage />

## Step 5: Create an Astro project

<DirectorySetup />

## Step 6: Migrate project code and dependencies to your Astro project

<Tabs
    defaultValue="manual"
    groupId= "step-6-migrate-project-code-and-dependencies-to-your-astro-project"
    values={[
        {label: 'Manual', value: 'manual'},
        {label: 'gcloud CLI', value: 'gcloud'},
    ]}>
<TabItem value="manual">

1. Open your Astro project Dockerfile. Update the Runtime version in first line to the version you selected for your Deployment in [Step 3](#step-3-create-astro-deployment). For example, if your Runtime version was 6.3.0, your Dockerfile would look like the following:

    ```Dockerfile
    FROM quay.io/astronomer/astro-runtime:6.3.0
    ```

   The `Dockerfile` defines the environment where all your Airflow components run. You can modify it to include build-time arguments for your Airflow environment, such as environment variables or credentials. For this migration, you only need to modify the Dockerfile to update your Astro Runtime version. 

2. Open your Astro project `requirements.txt` file and add all Python packages that you installed in your source Airflow environment. See [Google documentation](https://cloud.google.com/composer/docs/composer-2/install-python-dependencies#view-custom) for how to view a list of Python packages in your source Airflow environment. 

    :::warning 

    To avoid breaking dependency upgrades, Astronomer recommends pinning your packages to the versions running in your soure Airflow environment. For example, if you're running `apache-airflow-providers-snowflake` version 3.3.0 on Cloud composer, you would add `apache-airflow-providers-snowflake==3.3.0` to your Astro `requirements.txt` file.

    :::

3. Open your Astro project `dags` folder. Copy your DAG files to `dags` from either your source control platform or GCS Bucket.

4. If you used the [`plugins` folder](https://cloud.google.com/composer/docs/concepts/cloud-storage#folders_in_the_bucket) in your Cloud Composer storage bucket, copy the contents of this folder from your source control platform or GCS Bucket to your Astro project `/plugins` folder.

5. If you used the [`data` folder](https://cloud.google.com/composer/docs/concepts/cloud-storage#data) in Cloud Composer, copy the contents of that folder from your source control platform or GCS Bucket to your Astro project `include` folder.

</TabItem>
<TabItem value="gcloud">

1. Open your Astro project Dockerfile. Update the Runtime version in first line to the version you selected for your Deployment in [Step 3](#step-3-create-astro-deployment). For example, if your Runtime version was 6.3.0, your Dockerfile would look like the following:

    ```Dockerfile
    FROM quay.io/astronomer/astro-runtime:6.3.0
    ```

     The `Dockerfile` defines the environment where all your Airflow components run. You can modify it to include build-time arguments for your Airflow environment, such as environment variables or credentials. For this migration, you only need to modify the Dockerfile to update your Astro Runtime version. 

2. Open your Astro project in your terminal. Run the following command to copy your PyPI packages from GCC to your `requirements.txt` file:

    ```shell
    gcloud composer environments describe <your-gcc-project-name> --format="value(config.softwareConfig.pypiPackages)" > requirements.txt
    ```  

    Review the output in `requirements.txt` after running this command to ensure that all packages were imported on their own line of text.

    :::warning 

    To avoid breaking dependency upgrades, Astronomer recommends pinning your packages to the versions running in your soure Airflow environment. For example, if you're running `apache-airflow-providers-snowflake` version 3.3.0 on Cloud composer, you would add `apache-airflow-providers-snowflake==3.3.0` to your Astro `requirements.txt` file.

    :::

3. Run the following command to copy your DAGs from Cloud Composer to your `dags` folder:

    ```sh
    gcloud composer environments storage dags export --destination=dags
    ```

    Review your DAG files in `dags` after running this command to ensure that all DAGs were successfully exported. 

4. If you utilized the [`plugins` folder](https://cloud.google.com/composer/docs/concepts/cloud-storage#folders_in_the_bucket) in your Cloud Composer storage bucket, run the following command to copy your `plugins` folder contents to your Astro project:

    ```sh
    gcloud composer environments storage plugins export --destination=plugins
    ```  

    Review the contents of your Astro project `plugins` folder to ensure that all files were successfully exported. 

5. If you utilized the [`data` folder](https://cloud.google.com/composer/docs/concepts/cloud-storage#data) in Cloud Composer, run the following command to copy your `data` folder contents to your Astro project:

    ```sh
    gcloud composer environments storage data export --destination=include
    ```  
  
    Review the contents of your Astro project `include` folder to ensure that all files were successfully exported.   
  

</TabItem>
</Tabs>

## Step 7: Configure additional data pipeline infrastructure

The core migration of your project is now complete. Read the following topics to see whether you need to set up any additional infrastructure on Astro before cutting over your DAGs. 

### Set up CI/CD

If you used CI/CD to deploy code to your source Airflow environment, read the following documentation to learn about setting up a similar CI/CD pipeline for your Astro project:

- [Set-Up CI/CD](set-up-ci-cd.md)
- [CI/CD templates](ci-cd-templates/template-overview.md)

Similarly to GCC, you can deploy DAGs to Astro directly from a Google Cloud Storage (GCS) bucket. See [Deploy DAGs to from Google Cloud Storage to Astro](ci-cd-templates/gcs.md).

### Set up a secrets backend

If you currently store Airflow variables or connections in a secrets backend, you need to integrate your secrets backend with Astro to access those objects from your migrated DAGs. See [Configure a Secrets Backend](secrets-backend.md) for setup steps.

#### Instance permissions and trust policies

You can utilize Workload Identity or Service Account Keys to grant your Astro Deployment the same level of access to Google Services as your source Airflow environment. See [Connect GCP - Authorization options](connect-gcp.md).

## Step 8: Test locally and check for import errors

Depending on how thoroughly you want to test your Airflow environment, you have a few options for testing your project locally before deploying to Astro.

- In your Astro project directory, run `astro dev parse` to check for any parsing errors in your DAGs.
- Run `astro run <dag-id>` to test a specific DAG. This command compiles your DAG and runs it in a single Airflow worker container based on your Astro project configurations.
- Run `astro dev start` to start a complete Airflow environment on your local machine. After your project starts up, you can access the Airflow UI at `localhost:8080`. See [Troubleshoot your local Airflow environment](cli/run-airflow-locally.md).

Note that your migrated Airflow variables and connections are not available locally. You must deploy your project to Astro to test these resources. 

## Step 9: Deploy to Astro

<Deploy />

## Step 10: Cut over from your source Airflow environment to Astro

<FinalStep />
