---
sidebar_label: 'Migrate to Astro from GCC'
title: 'Migrating to Astro from GCC'
id: migrate-gcc
description: Get started on Astro by migrating your Airflow code from Google Cloud Composer (GCC).
---

import Intro from './migration-partials/intro.mdx';
import Prerequisites from './migration-partials/prereqs.mdx';
import Requirements from './migration-partials/requirements.mdx';
import Starship from './migration-partials/starship.mdx';
import Usage from './migration-partials/starship-usage.mdx';
import Workspaces from './migration-partials/workspaces.md';
import Deployments from './migration-partials/deployments.mdx';
import DAGs from './migration-partials/directory-setup.md';
import RuntimeVersionInfo from './migration-partials/runtime-version-info.mdx'
import FinalStep from './migration-partials/final-step.mdx'
import TestLocally from './migration-partials/test-locally.mdx'
import Deploy from './migration-partials/deploy.mdx'
import Additional from './migration-partials/additional.mdx'
import WrapUp from './migration-partials/wrap-up.mdx'

This is where you'll find instructions for migrating to Astro from another managed Airflow environment. This guide will
cover migrating an Airflow Instance from [Google Cloud Composer (GCC)](https://cloud.google.com/composer/docs/concepts/overview)

<Intro />

## Prerequisites

<Prerequisites />

You can additionally utilize `gcloud` CLI to expedite some steps in this guide.

## Prepare Source and Setup Astro Airflow

### Step 1: Install Astronomer Starship
<Starship />

#### Starship Compatability Matrix

| Source Airflow                 | Starship Plugin | Starship Operator |
|--------------------------------|-----------------|-------------------|
| Airflow 1.x                    | ❌               | ❌                 |
| Cloud Composer 1 - Airflow 2.x |                 | ✔️️               |
| Cloud Composer 2 - Airflow 2.x | ✔️️             |                   |

#### Installation

To install Starship to your Cloud Composer 1 or Cloud Composer 2 instance, you'll modify your instance's `PYPI Packages` tab.
There you can add `astronomer-starship` as an entry by clicking `Edit`.
You can find more [detailed instructions here.](https://cloud.google.com/composer/docs/composer-2/install-python-dependencies)

:::cli gcloud

You can accomplish this easily with the `gcloud` CLI, in a terminal (make sure to change `[GCC_NAME]` below):
```sh
gcloud composer environments update [GCC_NAME] --update-pypi-package=astronomer-starship
```

:::

We will cover usage of the Starship migration utility later in this document

### Step 2: Create Astro Workspace

<Workspaces />

### Step 3: Create Astro Deployment

<Deployments />

## Migrate Airflow Metadata
In this step we will migrate Airflow Metadata with Starship from Source Environment

Additionally - you should utilize Starship to test connections as you migrate them.

### Step 4: Use Starship to Migrate Airflow Connections and Variables
<Usage />

## Migrate DAGs
### Step 5: Initialize new repository skeleton
<DAGs />

### Step 6: Copy core Airflow code

1. Edit `Dockerfile` to match Astro Runtime version to customer’s Source environment Airflow version
    <RuntimeVersionInfo />

    :::info

    The `Dockerfile` is the backing system that all of your Airflow components run in.
    You can customize that in a variety of ways, and might
    modify it to copy special things like certificates or keys to be available in your environment.

    You shouldn't need to modify your `Dockerfile` beyond this, for the purposes of this migration, but may in the future.

    :::

2. Fill in your new `requirements.txt` file. You can do this by navigating to `PYPI Packages` in the source instance and copying those to the file
    :::cli gcloud

    You can easily copy this with the `gcloud` CLI:
    ```shell
    gcloud composer environments describe [GCC_NAME] --format="value(config.softwareConfig.pypiPackages)" > requirements.txt
    ```
    Make sure to review the output after running this command

    :::

3. Copy your `/dags` folder from source control or the GCS Bucket by navigating to either and downloading/moving the entire folder into your local Astro project.
    :::cli gcloud

    You can easily copy this with the `gcloud` CLI:
    ```
    gcloud composer environments storage dags export --destination=dags
    ```
    Make sure to review the output after running this command

    :::

### Step 7: Copy accessory Airflow code
Following the recommended [project structure](https://docs.astronomer.io/learn/managing-airflow-code#project-structure), we will move any files to our Astro project.

1. If you have utilized the `/plugins` folder in your Composer storage bucket, copy it from source control or GCS Bucket to the local `/plugins` folder
    :::cli gcloud

    You can easily copy this with the `gcloud` CLI:
    ```
    gcloud composer environments storage plugins export --destination=plugins
    ```
    Make sure to review the output after running this command

    :::
2. If you have utilized data storage with Cloud Composer, you can copy that into `/include`
    :::cli gcloud

    You can easily copy this with the `gcloud` CLI:
    ```
    gcloud composer environments storage data export --destination=include
    ```
    Make sure to review the output after running this command

    :::

### Step 8: Configure Additional Components
<Additional />

#### Instance Permissions and Trust Policies
You can utilize Workload Identity or Service Account Keys to grant your Astro Deployment the same level of access to Google Services as your source Airflow
by following the [Connect GCP - Authorization options](connect-gcp.md#authentication-options) documentation

### Step 9: Test locally and check for import errors
<TestLocally />

### Step 10: Deploy
<Deploy />

## Validate, Cut-over and Tune Process
<FinalStep />

## Wrapping up
<WrapUp />