---
sidebar_label: "Migrate to Astro from GCC"
title: "Migrating to Astro from GCC"
id: migrate-gcc
description: Get started on Astro by migrating your Airflow code from Google Cloud Composer (GCC).
---

import Intro from "./migration-partials/intro.mdx";
import Prerequisites from "./migration-partials/prereqs.mdx";
import Requirements from "./migration-partials/requirements.mdx";
import Starship from "./migration-partials/starship.mdx";
import Usage from "./migration-partials/starship-usage.mdx";
import Workspaces from "./migration-partials/workspaces.md";
import Deployments from "./migration-partials/deployments.mdx";
import DAGs from "./migration-partials/directory-setup.md";
import RuntimeVersionInfo from "./migration-partials/runtime-version-info.mdx";
import FinalStep from "./migration-partials/final-step.mdx";
import TestLocally from "./migration-partials/test-locally.mdx";
import Deploy from "./migration-partials/deploy.mdx";
import Additional from "./migration-partials/additional.mdx";
import WrapUp from "./migration-partials/wrap-up.mdx";

This is where you'll find instructions for migrating to Astro from another managed Airflow environment. This guide will
cover migrating an Airflow Instance from [Google Cloud Composer (GCC)](https://cloud.google.com/composer/docs/concepts/overview)

<Intro />

## Prerequisites

<Prerequisites />

You can additionally utilize `gcloud` CLI to expedite some steps in this guide.

## Prepare Source and Setup Astro Airflow

### Step 1: Install Astronomer Starship

<Starship />

| Source Airflow environment     | Starship plugin | Starship operator |
| ------------------------------ | --------------- | ----------------- |
| Airflow 1.x                    | ❌              | ❌                |
| Cloud Composer 1 - Airflow 2.x |                 | ✔️️                 |
| Cloud Composer 2 - Airflow 2.x | ✔️️               |                   |

To install the Starship plugin on your Cloud Composer 1 or Cloud Composer 2 instance, install the `astronomer-starship` package in your source Airflow environment. See [Install packages from PyPI](https://cloud.google.com/composer/docs/composer-2/install-python-dependencies#install-pypi)

:::tip

You can alternatively complete this installation with the `gcloud` CLI by running the following command:

```sh
gcloud composer environments update [GCC_NAME] --update-pypi-package=astronomer-starship
```

:::

### Step 2: Create Astro Workspace

<Workspaces />

### Step 3: Create Astro Deployment

<Deployments />

## Migrate Airflow Metadata

In this step we will migrate Airflow Metadata with Starship from Source Environment

Additionally - you should utilize Starship to test connections as you migrate them.

### Step 4: Use Starship to Migrate Airflow Connections and Variables

<Usage />

## Migrate DAGs

### Step 5: Initialize new repository skeleton

<DAGs />

### Step 6: Copy core Airflow code
1. Edit the `Dockerfile` 's Astro Runtime version to match the source Airflow version.
    **This must match the version of the Deployment you created in [Step 3](#step-3-create-astro-deployment)**.

    :::tip

    The Astro Runtime version is at the end of the line that begins with `FROM` -
    e.g. `FROM quay.io/astronomer/astro-runtime:1.2.3` would have a version of `1.2.3`

    The `Dockerfile` creates the backing environment that all of your Airflow components run in.
    You can customize that in a variety of ways, and might
    modify it to copy special things like certificates or keys to be available in your environment.

    You shouldn't need to modify your `Dockerfile` beyond the changes in this step, for the purposes of this migration, but may want to in the future.

   You shouldn't need to modify your `Dockerfile` beyond this, for the purposes of this migration, but may in the future.

   :::

2. Fill in your new `requirements.txt` file. You can do this by navigating to `PYPI Packages` in the source instance and copying those to the file

    :::cli gcloud

   You can easily copy this with the `gcloud` CLI:

   ```shell
   gcloud composer environments describe [GCC_NAME] --format="value(config.softwareConfig.pypiPackages)" > requirements.txt
   ```

   Make sure to review the output after running this command

   :::

3. Copy your `/dags` folder from source control or the GCS Bucket by navigating to either and downloading/moving the entire folder into your local Astro project.

    :::cli gcloud

   You can easily copy this with the `gcloud` CLI:

   ```
   gcloud composer environments storage dags export --destination=dags
   ```

   Make sure to review the output after running this command

   :::

### Step 7: Copy accessory Airflow code

Following the recommended [project structure](https://docs.astronomer.io/learn/managing-airflow-code#project-structure), we will move any files to our Astro project.

1. If you have utilized the `/plugins` folder in your Composer storage bucket, copy it from source control or GCS Bucket to the local `/plugins` folder

    :::cli gcloud

   You can easily copy this with the `gcloud` CLI:

   ```
   gcloud composer environments storage plugins export --destination=plugins
   ```

   Make sure to review the output after running this command

   :::

2. If you have utilized data storage with Cloud Composer, you can copy that into `/include`

   :::cli gcloud

   You can easily copy this with the `gcloud` CLI:

   ```
   gcloud composer environments storage data export --destination=include
   ```

   Make sure to review the output after running this command

   :::

### Step 8: Configure Additional Components

<Additional />

#### Instance Permissions and Trust Policies

You can utilize Workload Identity or Service Account Keys to grant your Astro Deployment the same level of access to Google Services as your source Airflow
by following the [Connect GCP - Authorization options](connect-gcp.md#authentication-options) documentation

### Step 9: Test locally and check for import errors

<TestLocally />

### Step 10: Deploy

<Deploy />

## Validate, Cut-over and Tune Process

<FinalStep />

## Wrapping up

<WrapUp />
