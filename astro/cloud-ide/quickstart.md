---
sidebar_label: Quickstart
title: Astro Cloud IDE quickstart
id: quickstart
---

:::caution

<!-- id to make it easier to remove: cloud-ide-preview-banner -->
The Cloud IDE is currently in _Public Preview_ and it is available to all Astro customers. It is still in development and features and functionality are subject to change. 

If you have any feedback, please submit it to the [Astro Cloud IDE product portal](https://portal.productboard.com/75k8qmuqjacnrrnef446fggj).
:::

This quickstart walks you through creating and running your first project through the Astro Cloud IDE.

## Prerequisites 

To complete this quickstart, you need:

- Workspace Editor permissions to an [Astro Workspace](manage-workspaces.md).
- Optional. A database hosted in one of the following services:
    - GCP BigQuery
    - Postgres (hosted)
    - Snowflake
    - AWS S3
    - Redshift 

If you don't provide a database, you can still complete the tutorial. However, you will not be able to test any SQL in your pipeline.

## Step 1: Log in and create a project

The Cloud IDE is accessible through the Cloud UI and is available to all Astro customers.

1. Log in to the Cloud UI and select a Workspace. 
2. Click **Cloud IDE** in the left sidebar. If you are the first person in your Workspace to use the Astro Cloud IDE, the **My Projects** page will be empty.

![Cloud IDE](/img/cloud-ide/project-list.png)

3. Click **+ Project**. Give your new project a name and a description, then click **Create**.

After you create your project, the Astro Cloud IDE opens your project home page. This page contains all menus for configuring your your project:

- The **Pipelines** tab stores all of the Python and SQL code that your project executes.
- The **Connections** tab stores Airflow connections for connecting your project to external services.
- The **Variables** tab stores Airflow variables for use in your pipeline code.
- The **Requirements** tab stores the required Python and OS-level dependencies for running your pipelines.

## Step 2: Create a pipeline

In the **Pipelines** tab, click **+ Pipeline**. Give your pipeline a name and a description, then click **Create**.

When you first run your project, your pipeline is built into a single DAG with the name you provide. Because of this, pipeline names must be unique within their project. They must also be a Python identifier, so they can't contain spaces or special characters. 

After clicking **Create**, the IDE opens the pipeline editor. This is where you'll write your pipeline code.

## Step 3: Create a Python cell 

Just like tasks are the building blocks of DAGs, cells are the building blocks of pipelines. Cells can be created with either Python or SQL code. For this tutorial, write a Python cell named `hello_world`.

1. Click **Create Cell**, then click **Python**. A new cell named `cell_1` appears on the page.
2. Click the cell's name and rename the cell `hello_world`.
3. Click the editing window where it says `# Write your code hereâ€¦`, then replace that line with the following code:
   
   ```python
   return "Hello, world!"
   ```

Your pipeline editor should look like the following:

![Hello world cell in the pipeline editor](/img/cloud-ide/hello-world.png)

## Step 4: Run your cell 

In the `hello_world` cell, click **Run** to execute a single run of your cell.

![Python Cell Logs](/img/cloud-ide/python-cell-logs.png)

When you run a cell, the Cloud IDE creates a temporary DAG that executes only an individual cell. Astro runs this DAG in your cloud and sends information about the cell run directly to the Cloud IDE.

The **Logs** tab contains all logs generated by the cell run, including Airflow logs and Python errors. The **Results** tab contains the contents of your Python console. If you click **Results** you should see the result of your successful cell run. 

## Step 5: Create a database connection

To create a SQL cell and execute SQL, create a database to run your SQL queries against. 

1. Click **Environment**, then click **Connections**. 

    ![Configure Connection](/img/cloud-ide/new-connection.png)

2. Click **NEW CONNECTION**.  
3. Choose one of the available connection types and configure all required values for the connection. Click **More options** to configure optional values for the connection.

  :::info

  SQL cell query results are stored as temporary tables in the database you're querying. The schema field in your connection object determines where these end up. Astronomer recommends setting this field to `TMP_ASTRO` for easy cleanup.

  :::

4. Optional. Click **Test Connection**. The Astro Cloud IDE runs a quick connection test and displays a message based on whether the test was a success. Regardless of whether the test succeeds, you can still create the connection.
5. Click **Create Connection**. You new connection appears in the **Connections** tab both in the pipeline editor and on your project homepage. You can use this connection with any future pipelines you create in this project. 

## Step 6: Create a SQL cell

You can now write and run SQL cells with your database connection.

1. Click **Create Cell**, then click **SQL**. A new cell appears on the page.
2. Click the cell's name and rename the cell `hello_sql`.
3. In the dropdown menu next to the cell's name, select your database connection.
4. Click the editing window where it says `-- Write your SQL query here`, then replace that line with the following code:

    ```sql
    SELECT 1 AS hello_world;
    ```

  :::tip

  You can also quick add a SQL cell with a given connection by clicking on the **+** button from the **Connections** tab in the **Environment** menu.

  :::

5. Optional. Click **Run** to test the SQL query. The results of your query appear in the **Results** tab.

## Step 7: Create dependencies between cells

You now have a Python cell and a SQL cell, but there's currently no logic to determine which task runs first in your DAG. You can create dependencies for these cells directly in the Astro Cloud IDE. Start by making `hello_sql` dependent on `hello_world` completing.

1. In the `hello_sql` cell, click the **Dependencies** icon.
2. Click `hello_world`. 
   
    ![Configure a dependency](/img/cloud-ide/create-dependency.png)   

3. To confirm that the dependency was established, click **Pipeline**. The **Pipeline** menu visualizes the dependencies between your cells.

    ![Dependency graph in the Pipelines menu](/img/cloud-ide/configured-dependency.png)   


## Step 7: Generate dependencies in code

One of the most powerful features of the Astro Cloud IDE is that it can automatically detect data dependencies in your cell code and restructure your pipeline based on those dependencies. This works for both Python and SQL cells.

To create a potential dependency to a Python cell, the upstream Python cell must end with a `return` statement. This means that you can create a downstream dependency from `hello_world`.

1. Create a new Python cell named `data_dependency`.
2. Add the following code to the cell:

    ```sh
    my_string = hello_world
    return my_string
    ``` 

    You can pass any value from a `return` statement into a downstream Python cell by calling the name of the upstream Python cell.

3. Open **Pipeline** menu to see that your dependency graph has updated:

    ![New dependency graph](/img/cloud-ide/data-dependency.png) 

You can generate dependencies between any two types of cells. See the following topics for more information about generating different types of dependencies. 

### Use the results of a SQL cell in a SQL cell

You can make a SQL cell downstream dependent of a SQL cell as long as the upstream SQL cell includes a `SELECT` statement. To do so, reference the name of the SQL cell in your SQL cell using double curly braces, also known as jinja templating.

The following cell is downstream of a SQL cell named `my_table`.

```sql
select * from {{my_table}} -- my_table is another SQL cell
```

### Use the results of a SQL cell in a Python cell

You can make a Python cell downstream dependent of a SQL cell as long as the SQL cell includes a `SELECT` statement. To do so, create a variable in the Python cell that points to the name of the SQL cell. SQL tables are automatically converted to pandas DataFrames for use in Python cells.

The following Python cell is downstream of a SQL cell named `my_sql_cell`. 

```python
df = my_sql_cell # my_sql_cell is a SQL cell which gets converted to a pandas DataFrame by default
df['col_a'] = df['col_a'] + 1
return df
```

### Use the results of a Python cell in a SQL cell

You can make a SQL cell downstream dependent of a Python cell as long as the Python cell returns a pandas DataFrame. To do so, reference the name of the Python cell in your SQL cell using double curly braces, also known as jinja templating. Pandas DataFrames are automatically converted to SQL tables for use in SQL cells.

The following SQL cell is downstream of a Python cell named `my_dataframe`. 

```sql
select * from {{my_dataframe}} -- my_dataframe is a Python cell
where col_a > 10
```

## Step 8: Run your pipeline 

Now that you've completed your pipeline, you can run it from beginning to end. To do so, click **Run** in the top right corner of your pipeline editing window. Cells are executed in order based on their dependencies. During the run, the **Pipeline** page shows which cells have been executed and which are still pending.

![Run Pipeline](/img/cloud-ide/run-pipeline.png)

## Step 9 Schedule your pipeline

Once you've verified that your pipeline works properly, you can schedule it to run on a regular basis. 

To set your pipeline's schedule, click **Schedule**. 

![Schedule Pipeline](/img/cloud-ide/schedule-menu.png)

The Cloud IDE currently supports scheduling with cron. You can either manually type in a cron string or click **EDIT** to open the cron builder, which is a simple UI for setting a cron schedule. After you make your selections and close out the cron builder, the Astro Cloud IDE loads a newly generated cron schedule.

When you finish configuring your schedule, click **Update Settings** to save your changes. 

Configuring your pipeline's schedule will not automatically run it on a scheduled basis. You must deploy your pipeline for it to run. See [Deploy a project](cloud-ide/deploy-project.md) for setup steps.


