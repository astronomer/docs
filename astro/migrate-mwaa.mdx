---
sidebar_label: "Migrate to Astro from MWAA"
title: "Migrating to Astro from MWAA"
id: migrate-mwaa
description: Get started on Astro by migrating your Airflow code from Amazon Managed Workflows for Apache Airflow (MWAA).
---

import Intro from "./migration-partials/intro.mdx";
import Prerequisites from "./migration-partials/prereqs.mdx";
import Starship from "./migration-partials/starship.mdx";
import Usage from "./migration-partials/starship-usage.mdx";
import Workspaces from "./migration-partials/workspaces.mdx";
import Deployments from "./migration-partials/deployments.mdx";
import DAGs from "./migration-partials/directory-setup.mdx";
import FinalStep from "./migration-partials/final-step.mdx";
import TestLocally from "./migration-partials/test-locally.mdx";
import Deploy from "./migration-partials/deploy.mdx";
import Additional from "./migration-partials/additional.mdx";

This is where you'll find instructions for migrating to Astro from another managed Airflow environment. This guide will
cover migrating an Airflow Instance from [Amazon Managed Workflows for Apache Airflow (MWAA)](https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html)

<Intro />

## Prerequisites

<Prerequisites />

You can additionally utilize the `aws` CLI to expedite some steps in this guide.

## Step 1: Install Astronomer Starship

<Starship />

| Source Airflow environment | Starship plugin | Starship operator |
| -------------------------- | --------------- | ----------------- |
| Airflow 1.x                | ❌              | ❌                |
| MWAA v2.0.2                |                 | ✔️️                 |
| MWAA v2.2.2                | ✔️️               |                   |
| MWAA v2.4.3                | ✔️️               |                   |

To install Starship to your MWAA instance as a plugin:

1. Download your existing `requirements.txt` file for your source Airflow environment from S3. See [AWS documentation](https://docs.aws.amazon.com/mwaa/latest/userguide/best-practices-dependencies.html#best-practices-dependencies-different-ways).
2. Add `astronomer-starship` on a new line to your `requirements.txt` file.
3. Re-upload the file to your S3 bucket.
4. Edit your Airflow environment to refer to the new version of this file.

:::tip

You can alternatively complete this installation with the `aws` CLI by running the following commands:

1. Run the following commands to set environment variables on your local machine:

    ```sh
    export MWAA_NAME="MWAA"
    export MWAA_BUCKET="MWAA BUCKET"
    ```

2. Run the following commands to install Starship:

    ```shell
    aws s3 cp "s3://$MWAA_BUCKET/requirements.txt" requirements.txt
    echo 'astronomer-starship' >> requirements.txt
    aws s3 cp requirements.txt "s3://$MWAA_BUCKET/requirements.txt"
    aws mwaa update-environment "$MWAA_NAME" --requirements-s3-object-version="$(aws s3api head-object --bucket=$MWAA_BUCKET --key=requirements.txt --query="VersionId")"
    ```

:::

## Step 2: Create an Astro Workspace

<Workspaces />

## Step 3: Create an Astro Deployment

<Deployments />

## Step 4: Use Starship to Migrate Airflow Connections and Variables

<Usage />

## Step 5: Create an Astro project

<DAGs />

## Step 6: Migrate project code and dependencies to your Astro project

1. Open your Astro project Dockerfile. Update the Runtime version in first line to the version you selected for your Deployment in [Step 3](#step-3-create-astro-deployment). For example, if your Runtime version was 6.3.0, your Dockerfile would look like the following:

    ```Dockerfile
    FROM quay.io/astronomer/astro-runtime:6.3.0
    ```

    The `Dockerfile` defines environment that all your Airflow components run in. You can modify it to make certain resources available to your Airflow environment like certificates or keys. For this migration, you only need to update your Runtime version. 

2. Open your Astro project `requirements.txt` file and add all Python packages from your source Airflow environment's `requirements.txt` file. See [AWS documentation](https://docs.aws.amazon.com/mwaa/latest/userguide/working-dags-dependencies.html) to find this file in your S3 bucket.

  :::tip 

  You can use the `aws` CLI to directly copy your PyPI packages from AWS to your Astro project `requirements.txt` file:

  ```sh
  aws s3 cp s3://[BUCKET]/requirements.txt requirements.txt
  ```  

  Review the output in `requirements.txt` after running this command to ensure that all packages were imported on their own line of text.    

  :::

3. Open your Astro project `dags` folder. Add your DAG files from either your source control platform or S3. 

  :::tip

  You can use the `aws` CLI to directly copy your DAGs from S3 to your `dags` folder, run:
  
  ```sh
  aws s3 cp --recursive s3://[BUCKET]/dags dags
  ```  
  
  Review your DAG files in `dags` after running this command to ensure that all DAGs were successfully exported. 

  :::

4. If you utilized the [`plugins` folder](https://cloud.google.com/composer/docs/concepts/cloud-storage#folders_in_the_bucket) in your MWAA project, copy the contents of this folder from your source control platform or S3 to your Astro project `/plugins` folder.

  :::tip
  
  You can use the `aws` CLI to copy your `plugins` folder contents directly to your Astro project, run:
  
  ```sh
  aws s3 cp --recursive s3://[BUCKET]/plugins.zip plugins.zip
  unzip plugins.zip
  ```
  
  Review the contents of your Astro project `plugins` folder after running this command to ensure that all files were successfully exported. 
  
  :::

## Step 7: Configure additional components

<Additional />

## Step 8: Test locally and check for import errors

<TestLocally />

## Step 9: Deploy to Astro

<Deploy />

## Step 10: Cut over from your source Airflow environment to Astro

<FinalStep />
