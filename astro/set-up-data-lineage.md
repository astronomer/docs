---
sidebar_label: 'Enable data lineage'
title: "Enable data lineage for external systems"
id: set-up-data-lineage
description: Configure your external systems to emit lineage data to Astro.
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

To generate lineage graphs for your data pipelines, you first need to configure your data pipelines to emit lineage data. Because lineage data can be generated in all stages of your pipeline, you can configure pipeline components outside of Astro, such as dbt or Databricks, to emit lineage data whenever they're running a job. Coupled with lineage data emitted from your DAGs, Astro generates a lineage graph that can provide context to your data before, during, and after it reaches your Deployment.

## Lineage architecture 

Lineage data is generated by [OpenLineage](https://openlineage.io/). OpenLineage is an open source standard for lineage data creation and collection. The OpenLineage API sends metadata about running jobs and datasets to Astro. Every Astro Organization includes an OpenLineage API key that you can use in your external systems to send lineage data back to your Control Plane.

![Diagram showing how lineage data flows to Astro](/img/docs/lineage-diagram.png)

Configuring a system to send lineage data requires:

- Installing an OpenLineage backend to emit lineage data from the system.
- Specifying your Organization's OpenLineage API endpoint to send lineage data to the Astro control plane.

:::tip

You can access this documentation directly from the **Lineage** tab in the Cloud UI. The embedded documentation additionally loads your Organization's configuration values, such as your OpenLineage API key and your Astro base domain, directly into configuration steps.

:::

### Retrieve your OpenLineage API key

To send lineage data from an external system to Astro, you must specify your Organization's OpenLineage API key in the external system's configuration. To find your Organization's API key:

1. In the Cloud UI, open the **Lineage** tab.
2. In the left menu, click **Integrations**:

    ![Location of the "Integrations" button in the Lineage tab of the Cloud UI](/img/docs/lineage-docs.png)

3. In **Getting Started**, copy the value below **OpenLineage API Key**.

For more information about how to configure this API key in an external system, review the Integration Guide for the system.

## Integration guides

<Tabs
    defaultValue="astronomer"
    values={[
        {label: 'Astronomer', value: 'astronomer'},
        {label: 'Databricks', value: 'databricks'},
        {label: 'Great Expectations', value: 'greatexpectations'},
        {label: 'Apache Spark', value: 'spark'},
        {label: 'dbt', value: 'dbt'},
    ]}>

<TabItem value="astronomer">

Lineage is configured automatically for all Deployments on Astro Runtime 4.2.0+. To add lineage to an existing Deployment that is running on a version of Astro Runtime that is lower than 4.2.0, upgrade to the latest version. For instructions, see [Upgrade Astro Runtime](upgrade-runtime.md).

>**Note:** If you don't see lineage features enabled for a Deployment on Runtime 4.2.0+, then you might need to [push code](https://docs.astronomer.io/cloud/deploy-code) to the Deployment to trigger the automatic configuration process.

To configure lineage on an existing Deployment on Runtime <4.2.0 without upgrading Runtime:

1. In your locally hosted Astro project, update your `requirements.txt` file to include the following line:

   ```
   openlineage-airflow
   ```

2. [Push your changes](https://docs.astronomer.io/cloud/deploy-code) to your Deployment.

3. In the Cloud UI, [set the following environment variables](https://www.astronomer.io/docs/cloud/stable/deploy/environment-variables) in your Deployment:

    ```
    AIRFLOW__LINEAGE__BACKEND=openlineage.lineage_backend.OpenLineageBackend
    OPENLINEAGE_NAMESPACE=<your-deployment-namespace>
    OPENLINEAGE_URL=https://<your-astro-base-domain>
    OPENLINEAGE_API_KEY=<your-lineage-api-key>
    ```

#### Verify

To view lineage metadata, go to the Organization view of the Cloud UI and open the **Lineage** tab. You should see your most recent DAG run represented as a data lineage graph in the **Lineage** page.

>**Note:** Lineage information appears only for DAGs that use operators that have extractors defined in the `openlineage-airflow` library, such as the `PostgresOperator` and `SnowflakeOperator`. For a list of supported operators, see [Data lineage Support and Compatibility](data-lineage-support-and-compatibility.md).

> **Note:** If you don't see lineage data for a DAG even after configuring lineage in your Deployment, you might need to run the DAG at least once so that it starts emitting lineage data.

</TabItem>

<TabItem value="databricks">

Use the information provided here to set up lineage collection for Spark running on a Databricks cluster.

#### Prerequisites

To complete this setup, you need:

- A Databricks cluster.

#### Setup

1. In your Databricks File System [(DBFS)](https://docs.databricks.com/data/databricks-file-system.html), create a new directory at `dbfs:/databricks/openlineage/`.
2. Download the latest OpenLineage `jar` file to the new directory. See [Maven Central Repository](https://search.maven.org/artifact/io.openlineage/openlineage-spark).
3. Download the `open-lineage-init-script.sh` file to the new directory. See [OpenLineage GitHub](https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/databricks/open-lineage-init-script.sh).
4. In Databricks, run this command to create a [cluster-scoped init script](https://docs.databricks.com/clusters/init-scripts.html#example-cluster-scoped-init-script) and install the `openlineage-spark` library at cluster initialization:

    ```sh
        dbfs:/databricks/openlineage/open-lineage-init-script.sh
    ```

5. In the cluster configuration page for your Databricks cluster, specify the following [Spark configuration](https://docs.databricks.com/clusters/configure.html#spark-configuration):

   ```sh
      bash
   spark.driver.extraJavaOptions -Djava.security.properties=
   spark.executor.extraJavaOptions -Djava.security.properties=
   spark.openlineage.url https://<your-astro-base-domain>
   spark.openlineage.apiKey <your-lineage-api-key>
   spark.openlineage.namespace <NAMESPACE_NAME> // Astronomer recommends using a meaningful namespace like `spark-dev`or `spark-prod`.
   ```

> **Note:** You override the JVM security properties for the spark _driver_ and _executor_ with an _empty_ string as some TLS algorithms are disabled by default. For a more information, see [this](https://docs.microsoft.com/en-us/answers/questions/170730/handshake-fails-trying-to-connect-from-azure-datab.html) discussion.

After you save this configuration, lineage is enabled for all Spark jobs running on your cluster.

#### Verify Setup

To test that lineage was configured correctly on your Databricks cluster, run a test Spark job on Databricks. After your job runs, open the **Lineage** tab in the Cloud UI and go to the **Explore** page. If your configuration is successful, you'll see your Spark job appear in the **Most Recent Runs** table. Click a job run to see it within a lineage graph.

</TabItem>

<TabItem value="dbt">

This guide outlines how to set up lineage collection for a dbt project.

#### Prerequisites

To complete this setup, you need:

- A [dbt project](https://docs.getdbt.com/docs/building-a-dbt-project/projects).
- The [dbt CLI](https://docs.getdbt.com/dbt-cli/cli-overview) v0.20+.
- Your Astro base domain.
- Your Organization's OpenLineage API key.

#### Setup

1. On your local machine, run the following command to install the [`openlineage-dbt`](https://pypi.org/project/openlineage-dbt) library:

   ```sh
   $ pip install openlineage-dbt
   ```

2. Configure the following environment variables in your shell:

   ```bash
   OPENLINEAGE_URL=https://<your-astro-base-domain>
   OPENLINEAGE_API_KEY=<your-lineage-api-key>
   OPENLINEAGE_NAMESPACE=<NAMESPACE_NAME> # Replace with the name of your dbt project.
                                          # Astronomer recommends using a meaningful namespace such as `dbt-dev` or `dbt-prod`.
   ```

3. Run the following command to generate the [`catalog.json`](https://docs.getdbt.com/reference/artifacts/catalog-json) file for your dbt project:

   ```bash
   $ dbt docs generate
   ```

4. In your dbt project, run the [OpenLineage](https://openlineage.io/integration/dbt/) wrapper script using the `dbt run` [command](https://docs.getdbt.com/reference/commands/run):

   ```bash
   $ dbt-ol run
   ```

#### Verify Setup

To confirm that your setup is successful, run a dbt model in your project. After you run this model, open the **Lineage** tab in the Cloud UI and go to the **Explore** page. If the setup is successful, the run that you triggered appears in the **Most Recent Runs** table.

</TabItem>

<TabItem value="greatexpectations">

This guide outlines how to set up lineage collection for a running Great Expectations suite.

#### Prerequisites

To complete this setup, you need:

- A [Great Expectations suite](https://legacy.docs.greatexpectations.io/en/latest/guides/tutorials/getting_started.html#tutorials-getting-started).
- Your Astro base domain.
- Your Organization's OpenLineage API key.

#### Setup

1. Update your `great_expectations.yml` file to add `OpenLineageValidationAction` to your `action_list_operator` configuration:

    ```yml
    validation_operators:
      action_list_operator:
        class_name: ActionListValidationOperator
        action_list:
          - name: openlineage
            action:
              class_name: OpenLineageValidationAction
              module_name: openlineage.common.provider.great_expectations
              openlineage_host: https://<your-astro-base-domain>
              openlineage_apiKey: <your-lineage-api-key>
              openlineage_namespace: <NAMESPACE_NAME> # Replace with your job namespace; Astronomer recommends using a meaningful namespace such as `dev` or `prod`.
              job_name: validate_my_dataset
    ```

2. Lineage support for GreatExpectations requires the use of the `ActionListValidationOperator`. In each of your checkpoint's xml files in `checkpoints/`, set the `validation_operator_name` configuration to `action_list_operator`:

    ```xml
    name: check_users
    config_version:
    module_name: great_expectations.checkpoint
    class_name: LegacyCheckpoint
    validation_operator_name: action_list_operator
    batches:
      - batch_kwargs:
    ```

#### Verify

To confirm that your setup is successful, open the **Lineage** tab in the Cloud UI and go to the **Issues** page. Recent data quality assertion issues appear in the **All Issues** table.

If your code hasn't produced any data quality assertion issues, use the search bar to search for a dataset and view its node on the lineage graph for a recent job run. Click the **Quality** tab to view metrics and assertion pass or fail counts.

</TabItem>

<TabItem value="spark">

This guide outlines how to set up lineage collection for Spark.

#### Prerequisites

To complete this setup you need:

- A Spark application.
- A Spark job.
- Your Astro base domain.
- Your Organization's OpenLineage API key.

#### Setup

In your Spark application, set the following properties to configure your lineage endpoint, install the [`openlineage-spark`](https://search.maven.org/artifact/io.openlineage/openlineage-spark) library, and configure an _OpenLineageSparkListener_:

   ```python
   SparkSession.builder \
     .config('spark.jars.packages', 'io.openlineage:openlineage-spark:0.2.+')
     .config('spark.extraListeners', 'io.openlineage.spark.agent.OpenLineageSparkListener')
     .config('spark.openlineage.host', 'https://<your-astro-base-domain>')
     .config('spark.openlineage.apiKey', '<your-lineage-api-key>')
     .config('spark.openlineage.namespace', '<NAMESPACE_NAME>') # Replace with the name of your Spark cluster.
     .getOrCreate()                                             # Astronomer recommends using a meaningful namespace such as `spark-dev` or `spark-prod`.
   ```

#### Verify

To confirm that your setup is successful, run a Spark job after you save your configuration. After you run this model, open the **Lineage** tab in the Cloud UI and go to the **Explore** page. Your recent Spark job run appears in the **Most Recent Runs** table.

</TabItem>
</Tabs>
