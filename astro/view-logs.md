---
title: 'View Deployment logs'
sidebar_label: 'View Deployment logs'
id: view-logs
description: View logs for your Deployments both locally and on Astro.
---

View task and Airflow component logs to troubleshoot your data pipelines and better understand the health of both your tasks and their execution environment.

Airflow has two different log types: 

- _Component logs_ record the performance of your Airflow components.
- _Task logs_ records events emitted by your DAG as each task executes.

## Airflow Component Logs

Airflow has four core components: the scheduler, triggerer, worker, and webserver. Each component records its process in component logs. These logs can be used to monitor overall performance, troubleshoot errors, and optimize resources.

- _Scheduler logs_ describe the performance of the scheduler, which is responsible for scheduling and queueing task runs. For more information on configuring the scheduler on Astro, see [Scheduler resources](deployment-settings.md#scheduler-size).

- _Triggerer logs_ describe the performance of the triggerer, the Airflow component responsible for running triggers and signaling tasks to resume when their conditions have been met. The triggerer is used exclusively for tasks that are run with [deferrable operators](https://docs.astronomer.io/learn/deferrable-operators).

- _Worker logs_ are generated by Celery Workers and can help you monitor task execution to optimize performance. This type of log is not available when using the Kubernetes Exeuctor.

- _Webserver logs_ relate to the health and performance of [the Airflow UI](https://docs.astronomer.io/learn/intro-to-airflow#airflow-components). If the Airflow UI at any point does not load, for example, webserver logs might indicate why.

### Airflow component log levels 

Logs and messages might also be associated with one of the following _log levels_: 

- **Error**: Emitted when a process fails or does not complete. For example, these logs might indicate a missing DAG file, an issue with your scheduler's connection to the Airflow database, or an irregularity with your scheduler's heartbeat.
- **Warn**: Emitted when Airflow detects an issue that may or may not be of concern but does not require immediate action. This often includes deprecation notices marked as `DeprecationWarning`. For example, Airflow might recommend that you upgrade your Deployment if there was a change to the Airflow database or task execution logic.
- **Info**: Emitted frequently by Airflow to show that a standard scheduler process, such as DAG parsing, has started. These logs are frequent and can contain useful information. If you run dynamically generated DAGs, for example, these logs will show how many DAGs were created per DAG file and how long it took the scheduler to parse each of them.

### View Airflow component logs in the Cloud UI

You can access scheduler, triggerer, and task logs in the Cloud UI to find the past 24 hours of logs for any Deployment on its **Logs** page. 

1. In the Cloud UI, select a Workspace and then a Deployment.

2. Click the **Logs** tab.

The maximum number of lines returned is 10,000, with 50 results displayed per page. If there are no logs available for a given Deployment, the following message appears:

```text
No matching events have been recorded in the past 24 hours.
```

Typically, this indicates that the Deployment you selected does not currently have any DAGs running.

#### Filter options

You can use the following options to specify the types of logs or messages that you want to view. 

- **String search**: Enter a string, keyword, or phrase to find in your logs. You can also search with suffix wildcards by adding a `*` to your search query. For example, `acti*` returns results that include `action` and `acting`. The string search does not include fuzzy matching, so misspelled strings or incomplete strings without a wildcard, `*`, return zero results.

- **Time range**: Filter the logs displayed based on time. 

- **Log type**: Filter based on whether the log message is from a scheduler, worker, webserver, or trigger. 

### View Airflow component logs locally

To show logs for your Airflow scheduler, webserver, or triggerer locally, run the following Astro CLI command:

```sh
astro dev logs
```

After you run this command, the most recent logs for these components appear in your terminal window.

By default, running `astro dev logs` shows logs for all Airflow components. To see logs only for a specific component, add any of the following flags to your command:

- `--scheduler`
- `--webserver`
- `--triggerer`

To continue monitoring logs, run `astro dev logs --follow`. The `--follow` flag ensures that the latest logs continue to appear in your terminal window. For more information about this command, see [CLI Command Reference](cli/astro-dev-logs.md).

## Airflow task logs

Airflow task logs for both [local Airflow environments](https://docs.astronomer.io/learn/logging#log-locations) and Deployments on Astro are available in the Airflow UI and Cloud UI. Task logs can help you troubleshoot a specific task instance that failed or retried.

Task logs for Astro Deployments are retained for 90 days. The task log retention policy is not currently configurable.

### Airflow task log levels 

Similar to the Airflow component log levels, task logs might also be associated with one of the following log levels, that you can search or filter with: 

- **Error**
- **Warn**
- **Info**
- **Debug**
- **Critical**

### View task logs on the Cloud UI

To access task logs from the Cloud UI:

1. In the Cloud UI, select a Workspace.
2. Click **DAGs**.
3. Click the DAG you want to view task logs for. 
4. Click a task run in the DAG run grid.
5. Click the **Logs** tab to switch from **Graph** view.

### View task logs in the Airflow UI

1.  Access the Airflow UI. 
  * To access the Airflow UI for a Deployment, open the Deployment in the Cloud UI and click **Open Airflow**. 
  * To access the Airflow UI in a local environment, open a browser and go to `http://localhost:8080`.
2. Click a DAG.
3. Click **Graph**.
4. Click a task run.
5. Click **Instance Details**.
6. Click **Log**.

## Export task logs to Datadog

If your organization uses data dog as a centralized observability plane, you might want to forward Airflow task logs from a Deployment to [Datadog](https://www.datadoghq.com/) using a Datadog API key. Complete the following setup to view Airflow task logs from your Datadog instance.

### Prerequisites

- Your Deployment must be running Astro Runtime 9 (AWS) or 9.1 (Azure and GCP). See [Upgrade Astro Runtime](upgrade-runtime.md).

### Setup

1. Create a new Datadog API key or copy an existing API key. See [API and Application Keys](https://docs.datadoghq.com/account_management/api-app-keys/).
2. Set the following [environment variable](environment-variables.md) on your Deployment:

    - **Key 1**: `DATADOG_API_KEY`
    - **Value 1**: Your Datadog API key.

    - **Key 2**: `ASTRO_DATADOG_TASK_LOGS_ENABLED`
    - **Value 2**: `true`

    Select the **Secret?** checkbox for `DATADOG_API_KEY`. This ensures that your Datadog API key is saved securely and is not available to Workspace users in plain text.

  :::info

  By default, the Astro Datadog integration also sends a Deployment's [Airflow metrics](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.html) to Datadog. To send only task logs to Datadog, additionally set the following environment variable:

    - **Key**: `ASTRO_DATADOG_METRICS_DISABLED`
    - **Value**: `true`

  :::

3. (Optional) Set the following [environment variable](environment-variables.md) on your Deployment to send your metrics to a specific [Datadog site](https://docs.datadoghq.com/getting_started/site/):

    - **Key**: `DATADOG_SITE`
    - **Value**: Your Datadog **Site Parameter**. For example, `datadoghq.com`.
   
5. Click **Save variable**.

Astro also supports exporting [Airflow metrics](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.html) to Datadog. See [Export Airflow metrics to Datadog](deployment-metrics.md#export-airflow-metrics-to-datadog).
