---
sidebar_label: 'Deploy code'
title: 'Deploy code to Astro'
id: deploy-code
description: Deploy Airflow DAGs to Astro.
---

import {siteVariables} from '@site/src/versions';

Pushing your Astro project, including your DAG code, to a Deployment with the Astro CLI is the foundation for managing changes on Astro. It also serves as the foundation for any [CI/CD](ci-cd.md)-based deploys.

Follow the steps in this document to push your Astro project to a Deployment. To deploy only DAG code changes to Astro, see [DAG-only deploys](#dag-only-deploys).

## Prerequisites

- The [Astro CLI](cli/overview.md) installed in an empty directory.
- An Astro Workspace with at least one [Deployment](create-deployment.md).
- An [Astro project](create-project.md).
- [Docker](https://www.docker.com/products/docker-desktop).

## Step 1: Authenticate to Astro

Once you've tested your Astro project locally, you're ready to push it to a Deployment. To start, authenticate to the Astro CLI by running:

```sh
astro login
```

After running this command, you will be prompted to open your web browser and log in to the Cloud UI. Once you complete this login, you will be automatically authenticated to the CLI.

:::tip

If you have [Deployment API key](api-keys.md) credentials set as OS-level environment variables on your local machine, you can deploy directly to Astro without needing to manually authenticate. This setup is required for automating code deploys with [CI/CD](ci-cd.md).

:::

## Step 2: Push your Astro project to an Astro Deployment

To deploy your Astro project, run:

```sh
astro deploy
```

This command returns a list of Deployments available in your Workspace and prompts you to pick one.

After you select a Deployment, the CLI parses your DAGs to ensure that they don't contain basic syntax and import errors. This test is equivalent to the one that runs during `astro dev parse` in a local Airflow environment. If any of your DAGs fail this parse, the deploy to Astro also fails.

If your code passes the parse, the Astro CLI builds your Astro project directory into a new Docker image and then pushes the image to your Deployment on Astro. To force a deploy even if your project has DAG errors, you can run `astro deploy --force`.

:::tip

To validate your code before deploying it to Astro, you can run `astro deploy --pytest`. Adding the `--pytest` flag makes the CLI run all tests in your project's `tests` directory using [pytest](https://docs.pytest.org/en/7.0.x/contents.html). If any of these tests fail, your code deploy also fails. This can help you prevent your team from deploying DAGs to Astro that aren't production-grade.

For more information about using Pytest, see [Test and troubleshoot locally](test-and-troubleshoot-locally.md#test-dags-locally-with-pytest).

:::

## Step 3: Validate your changes

After the deploy completes, Docker image information for your Deployment is available in the **Image tag** field in the footer of the Airflow UI. Depending on how your organization deploys to Astro, the **Image tag** field displays a unique identifier generated by a Continuous Integration (CI) tool or a timestamp generated by the Astro CLI on `astro deploy`. The **Image tag** field in the Airflow UI footer identifies the Docker image running on the webserver of your Deployment. This information can help you determine if your deploy was successful, but it does not identify the Docker image running on your scheduler, triggerer, or workers. To confirm a code push, contact [Astronomer support](https://cloud.astronomer.io/support).

1. In the Cloud UI, select a Workspace and then select the Deployment you pushed code to.
2. Click **Open Airflow**.
3. Scroll to the bottom of the page and view the **Image tag** information in the footer:

    ![Docker image information](/img/docs/image-tag-airflow-ui-astro.png)

## What happens during a project deploy

When you deploy code to Astro, your Astro project is built into a Docker image. This includes system-level dependencies, Python-level dependencies, DAGs, and your `Dockerfile`. It does not include any of the metadata associated with your local Airflow environment, including task history and Airflow connections or variables that were set locally. This Docker image is then pushed to all containers running the Apache Airflow application on Astro.

![Deploy code](/img/docs/deploy-architecture.png)

With the exception of the Airflow webserver and some Celery workers, Kubernetes gracefully terminates all containers during this process. This forces them to restart and begin running your latest code.

If you deploy code to a Deployment that is running a previous version of your code, then the following happens:

- Tasks that are `running` will continue to execute on existing Celery workers and will not be interrupted unless the task does not complete within 24 hours of the code deploy.
- One or more new worker(s) will spin up alongside your existing workers and immediately start executing scheduled tasks based on your latest code.

    These new workers will execute downstream tasks of DAG runs that are in progress. For example, if you deploy to Astronomer when `Task A` of your DAG is running, `Task A` will continue to run on an old Celery worker. If `Task B` and `Task C` are downstream of `Task A`, they will both be scheduled on new Celery workers running your latest code.

    This means that DAG runs could fail due to downstream tasks running code from a different source than their upstream tasks. DAG runs that fail this way need to be fully restarted from the Airflow UI so that all tasks are executed based on the same source code.

Astronomer sets a grace period of 24 hours for all workers to allow running tasks to continue executing. This grace period is not configurable. If a task does not complete within 24 hours, its worker will be terminated. Airflow will mark the task as a [zombie](https://airflow.apache.org/docs/apache-airflow/stable/concepts/tasks.html#zombie-undead-tasks) and it will retry according to the task's retry policy. This is to ensure that our team can reliably upgrade and maintain Astro as a service.

:::tip

If you want to force long-running tasks to terminate sooner than 24 hours, specify an [`execution_timeout`](https://airflow.apache.org/docs/apache-airflow/stable/concepts/tasks.html#timeouts) in your DAG's task definition.

:::

## DAG-only deploys

Running `astro deploy` without additional flags deploys your entire Astro project, including your DAG code, Dockerfile, and dependencies. There are some scenarios where you need to deploy only your Astro project's `dags` directory:

- You work on an Astro project where one team member is responsible for project configuration changes and another team member is responsible for DAG authoring.
- You want to deploy DAGs without building a Docker image or using the Astro CLI. For example, you upload DAGs to a shared S3 bucket.
- You want to deploy DAG changes without downtime to your Deployment. 

In these cases, you can enable DAG-only deploys on a Deployment.

### Enable DAG-only deploys

```sh
astro deployment update --dag-deploy enable
```

The CLI prompts you to select a Deployment to enable the feature on. After you enable the feature, anyone with access to the Deployment can deploy DAGs.

:::caution 

Enabling DAG deploys pauses all DAGS in a Deployment. Any active DAG runs will finish, but new DAG runs will not be scheduled until you unpause the DAGs from the Deployment's Airflow UI.

:::

### Trigger a DAG-only deploy

Run the following command to deploy only your current `dags` directory to a Deployment:

```sh
astro deploy --dags
```

This command pushes DAGs to a component on Astro that handles loading DAGs into your Airflow environment. DAG only deploys do not disrupt any active task runs, resulting in zero downtime for the Deployment. If you deploy changes to a currently running DAG, any active task runs finish using code from before the Deploy. Any new task runs are scheduled using code from after the deploy. 

DAG runs could fail due to downstream tasks running code from a different source than their upstream tasks. DAG runs that fail this way need to be fully restarted from the Airflow UI so that all tasks are executed based on the same source code.

## Related documentation

- [Develop your project](develop-project.md)
- [Set environment variables](environment-variables.md)
- [Known limitations](known-limitations.md)
