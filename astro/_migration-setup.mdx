### Step 2: Create Astro Workspace

Astro has Workspaces - which are collections of Deployments that can be accessed by a specific group of users. 

You can [read this document](manage-workspaces.md) that goes into more detail about creating workspaces with the Astro UI

Afterwards, make sure to [add users](add-user.md) to your new Workspace, and [refer to this document](user-permissions.md) 
for details on user permissions.

To complete this step - create a workspace to hold your migrated Airflow Deployments.

### Step 3: Create Astro Deployment

Deployments in Astro are your Airflow Environments. There are a number of strategies and considerations while creating 
your Astro Deployment that will mirror your migration environment.

You will want to retrieve the following information from your source Airflow Environment:

- Name
- Airflow Version
- Environment Class or Size
- Number of Schedulers
- Minimum Number of Workers
- Maximum Number of Workers
- Execution Role Permissions
- Airflow Configuration modifications
- Environment Variables

:::info
A mapping of Astronomer Runtime releases to Airflow versions is available [in the Release Notes](https://docs.astronomer.io/astro/release-notes)
This table is included as a quick but potentially incomplete reference. 
Where exact version matches are not available, the nearest version was picked.

| Airflow Version | Runtime Version  |
|-----------------|------------------|
| 2.0.2           | 3.0.4 (AF 2.1.1) |
| 2.2.2           | 4.2.9 (AF 2.2.5) |
| 2.4.3           | 6.3.0            |
:::

:::info
Scheduler size is measured in AUs, our recommended equivalents are:

|        | AUs |
|--------|-----|
| Small  | 5   |
| Medium | 10  |
| Large  | 15  |
:::

:::note
You will likely only have one instance type associated with your cluster at this point, later on in the process you can  [add other instance types](modify-cluster#manage-worker-types).
:::

You can create your Deployment in the Astro UI or Astro CLI.

<Tabs defaultValue="ui" groupId="deployment" values="{[ {label: 'Option 1: Astro UI', value: 'ui'}, {label: 'Option 2: Astro CLI, Deployments-as-Code', value: 'cli'} ]}">
<TabItem value="ui">
To create your target Astro Deployment, refer to [Create a Deployment](create-deployment.md) and [Configure Deployment Resources](configure-deployment-resources.md).
1) **Name your Deployment** to match your existing environment, optionally give it a description
2) Pick the **Cluster** that was previously activated with your Astronomer Representative
3) Select the appropriate **Astronomer Runtime** version
4) Set your **Worker Type**, if available, or pick the default
5) Ensure you set your **Worker Count** minimum and maximum values are set, per the source environment
6) Ensure the **number of Schedulers** is set, per the source environment
7) Adjust the **Scheduler Size**, roughly approximate to the source environment Size by converting to AUs based on the reference above
8) After creating your Deployment - you can add [Environment Variables](environment-variables.md) in the `Variables` Tab of your new Deployment
9) You can set Airflow Configuration variables as [Environment Variables](environment-variables.md) in the `Variables` Tab of your new Deployment
10) Set the [Alert Email](configure-deployment-resources#add-or-delete-a-deployment-alert-email) to your email address, or a good contact for the Airflow instance
</TabItem>
<TabItem value="cli">
To create your target Astro Deployment via the CLI, refer to [Manage Deployments as code](manage-deployments-as-code). 
We will begin by creating a file called `config.yaml` in a new directory.
Fill it with the following:
```yaml
deployment:
    environment_variables:
        - is_secret: false 
          key: FOO 
          value: BAR
    configuration:
        name: [NAME]
        description: [DESCRIPTION]
        runtime_version: [RUNTIME VERSION]
        dag_deploy_enabled: true
        scheduler_au: [SCHEDULER AU]
        scheduler_count: [NUM SCHEDULERS]
        cluster_name: [CLUSTER NAME] 
        workspace_name: [WORKSPACE NAME] 
    worker_queues:
        - name: default
          max_worker_count: [MAX WORKER COUNT] 
          min_worker_count: [MIN WORKER COUNT]
          worker_concurrency: 16
          worker_type: [WORKER TYPE]
    alert_emails:
        - [ALERT EMAIL] 
```
1) **Name your Deployment** to match your existing environment by replacing `[NAME]`, optionally replace `[DESCRIPTION]`
2) Replace `[CLUSTER]` with the name of the **Cluster** that was previously activated with your Astronomer Representative
3) Select the appropriate **Astronomer Runtime** version for `[RUNTIME VERSION]`
4) Set your **Worker Type** in `[WORKER TYPE]` by picking the default, unless you have already [added other instance types](modify-cluster#manage-worker-types)
:::info
The default instance type for a new cluster are: 
| AWS       | GCP           | AZ              |
|-----------|---------------|-----------------|
| M5.XLARGE | E2-STANDARD-4 | STANDARD_D4D_V5 |
A more complete reference is available in the [Cluster Settings Reference](resource-reference-aws.md) pages.
:::
5) Ensure you set your **Worker Count** minimum with `[MIN WORKER COUNT]` and maximum with `[MAX WORKER COUNT]`, per the source environment
6) Ensure the **number of Schedulers** is set on `[NUM SCHEDULERS]`, per the source environment
7) Adjust the **Scheduler Size**, roughly approximate to the source environment Size by converting to AUs based on the reference above
8) Add [Environment Variables](environment-variables.md) `environment_variables` section. 
9) You can set Airflow Configuration variables as [Environment Variables](environment-variables.md) in the `environment_variables` section.
10) Set `[ALERT EMAIL]` to your email address, or a good contact for the Airflow instance. More information [here](configure-deployment-resources#add-or-delete-a-deployment-alert-email)
After you have finished creating `config.yaml` file, you can create the deployment with:
```shell
astro deployment create --deployment-file config.yaml
```
</TabItem>
</Tabs>

## Migrate Airflow Metadata

### Step 4: Migrate Airflow Metadata with Starship from Source Environment

- Migrate Connections
- Migrate Variables
- Test connections via Starship when possible
- Migrate Env Variables
    - Note: not all Env Vars are [configurable on Astro](https://docs.astronomer.io/astro/platform-variables)
- Create pools, as needed

## Migrate DAGs 
### Initialize new repository skeleton
1. Using the same name as the chosen environment, create a new directory for your project
2. Initialize a git repository with `git init`, using the git CLI and/or your Source Control Toolâ€™s UI
3. Initialize the Astro Project with `astro dev init`

### Copy core Airflow code
1. Copy `/dags` folder from source control or blob storage
2. Edit `Dockerfile` to match Astro Runtime version to customerâ€™s Source environment Airflow version
    - Some Airflow versions are not supported - lowest supported version is Airflow 2.2.x
    - Reference runtime release notes [here](https://docs.astronomer.io/astro/runtime-release-notes) to determine which Airflow version corresponds with which runtime version

    ðŸ’¡ At this point, understand the `Dockerfile` and itâ€™s purpose, modification beyond setting the version will not be needed in these migrations.

3. Add `requirements.txt` , as needed.

### Copy accessory Airflow code
Following recommended [project structure](https://docs.astronomer.io/learn/managing-airflow-code#project-structure):

- Copy `/plugins` folder from source control or blob storage, as needed

### Configure Additional Components
- Add CICD if already being used in Source environment
- Add Secrets Backend if already being used in Source environment
- Add Trust Policies between Astro and Cloud Roles, as needed.

### Test locally and check for import errors
Test the environment locally with the CLI using `astro dev parse`  and `astro dev start` - inspect errors in the CLI and/or the localhost webserver

### Deploy
Deploy with astro deploy 

## Test and Cutover
### **Test and Cutover**
- Confirm successful migration of connections & variables in Target environment
- Validate and test DAGs in Target environment
- Utilize Starship in Source Environment to pause DAGs in Source and unpause DAGs in Target
### **Tune instance**
- Monitor analytics as DAGs turn on
- Add new instance types to match worker size from source
- Add any intentionally set Airflow Configuration settings
    - Note: not all configurations are [configurable on Astro](https://docs.astronomer.io/astro/platform-variables)
- Add `DAGs-only Deployment` , if desired
### Continue and repeat
- Customer continues to **Test and Cutover DAGs** on Astro with support from Astro Data Engineers via Slack, engaging in additional calls, as needed.
- Repeat Migration for all Environments after initial airflow environment is fully migrated